#! /usr/bin/perl -w

# This is a quick hack using the loggator log parser in progress
# to parse gmjobs.log and post results directly to lcgce's mysql.

# To be replaced with a loggator export directive once loggator is ready.

# It has a critical condition on time to be run from cron:
# it MUST run between /etc/cron.d/edg-apel-pbs-parser and
# and /etc/cron.d/edg-apel-publisher to ensure pbs data is parsed
# by apel and our data from gmjobs.log is published with
# the regular publish push.

# It gets all data on previous run from checking with lcgce's mysql
# so it should republish missing things.

# Todo: are all options supported?
# fix slurm dates in state
# remove slurm logs and store them in /tmp
# be conservative with large slurm time spans (check for number of hits, use day spans)
# Finish transaction size support.
# fix local time in logs and make logs use real time, not start of script time
# make rewind less hackish and per log
# time to trasition and refactor?
# for slurm, parse /etc/slurm/slurm.conf to get the control machine

#Remember:
# GkRecords <- globus gatekeeper logs - obsoleted
# EventRecords <- brenta pbs (torque) logs
# BlahdRecords <-  grid-jobmap* in /opt/edg/var/gatekeeper <- globus submit script
# MessageRecords <- /var/log/message
# consider /var/spool/nordugrid/map/*/pool for uid -> vo mapping

use strict; use warnings;

use Getopt::Long ;

use Data::Dumper ;
use DateTime ;
use DBI;
use File::Temp qw( :mktemp);

use lib 'lib';
use Loggator::Confer ;
use Loggator::Parser ;
#use Loggator::Storage ; #wishful future


#setup

END { savestate(); 1; }

use vars qw($state);
my $version = '0.12';
my $progname = $0; $progname =~ s{(.*/)?([^/]+)$}{$2} ;

my $config = 'gmjobs-metrics-exporter.rc' ;
my $logfile = 'gmjobs-metrics-exporter.log' ;
my $statefile = 'gmjobs-metrics-exporter.state' ;
my $nocommit = 0;
my $blahmode = 1;
my $from; my $to;
my @limit = (); my %limit;
my $reparse= 0;
my $verbose = 0;
my $debug = 0;
my $withundo = 0;
my $help = 0;
my $jumpto = undef;

my $hostname ;
my $port ;
my $database ;
my $user ;
my $passwd ;
my $transaction ;

my $sitename;
my $dns_suffix;
my $joburi_prefix;
my $queuename;
my $batchname;
my $hideowner;

Getopt::Long::Configure qw(no_ignore_case);
GetOptions ( 'config|c=s'   => \$config,
	     'logfile|l=s'  => \$logfile,
             'statefile|s=s'=> \$statefile,
	     'nocommit|n'   => \$nocommit,
	     'reparse|r'    => \$reparse,
	     'jumpto|j=s'   => \$jumpto,
	     'blahmode|B'   => \$blahmode,
	     'limit|L=s'    => \@limit,
	     'from=s'       => \$from,
	     'to=s'         => \$to,
	     'verbose|v'    => \$verbose,
	     'debug|d'      => \$debug,
	     'withundo|u=s' => \$withundo,
	     'hostname|H=s' => \$hostname,
	     'port|p=i'     => \$port,
	     'database|b=s' => \$database,
	     'username|U=s' => \$user,
	     'password|P=s' => \$passwd,
	     'transaction|t=i' => \$transaction,
	     'sitename=s'   => \$sitename,
	     'dns=s'        => \$dns_suffix,
	     'joburi_prefix=s' => \$joburi_prefix,
	     'queuename=s'  => \$queuename,
	     'batchname=s'  => \$batchname,
	     'hideowner'    => \$hideowner,
	     'help|h'       => \$help,
	   );

usage () and exit (0) if $help ;

# Set up output
open our $log, ">>", $logfile
  or die "Can't open logfile $logfile, @!\n";
$| = 1, select $_ for select $log; #log to autoflush :-)
open $state, "<", $statefile
  or warn "Can't open state file $statefile for reading, @!\n" unless $reparse ;

if (scalar @limit) {
  @limit = map { split /,/, $_ } @limit; # allow comma-separated lists
  printlog ("Processing limited to the following logs: " . join(', ', @limit) . "\n");
  %limit = map { $_ => 1 } @limit;       # prepare a hash for easy access
}


# Configuration
my $conf = Loggator::Confer->new($config);
$conf->setlog($log);
$conf->process();

printlog ("Configuration processing of $config finished.\n");

$hostname    ||= $conf->{backends}{exporter}{args}{hostname};
$port        ||= $conf->{backends}{exporter}{args}{port};
$database    ||= $conf->{backends}{exporter}{args}{database};
$user        ||= $conf->{backends}{exporter}{args}{user};
$passwd      ||= $conf->{backends}{exporter}{args}{password};
$transaction ||= $conf->{backends}{exporter}{transaction};

$sitename      ||= $conf->{site}{sitename};
$dns_suffix    ||= $conf->{site}{dns_suffix};
$joburi_prefix ||= $conf->{site}{joburi_prefix};
$queuename     ||= $conf->{site}{queuename};
$batchname     ||= $conf->{site}{batchname};


#my $storage = Loggator::Storage->new( $conf->{backends}, $conf->{confs}  );
#$DB::single = 2;

printlog("Config info:\nHost: $hostname\nPort: $port\nConnection $database:$user (tranactions of $transaction)\n\nSite: $sitename\nDNS suffix: $dns_suffix\nJobURI prefix: $joburi_prefix\n
Queuename: $queuename, Batchname $batchname\n") if $verbose;

my $prevpos ;
my $lastpos ;
my $previd ;
my $startfrom ;
my $dt ;
my $undofile ;

printlog ("\u$progname v $version starting.\n");

my $dbh = DBI->connect("DBI:mysql:database=$database;host=$hostname;port=$port",
 		       $user, $passwd,
		      { RaiseError => 1, AutoCommit => 0 },
		      )
  or die "Can't connect to mysql at $hostname:$port as $user.\n"
  if $hostname;
printlog ("! MySQL hostname not set - dry run!\n") unless $hostname;
warn "MySQL hostname not set - dry run!\n" unless $hostname;

if ($withundo) {
    my $start = DateTime->now();
    my $date = $start->ymd() ;
    my $time = $start->hms() ;
    $withundo =~ s{%d}{$date}g ;
    $withundo =~ s{%t}{$time}g ;
    open ($undofile, '>>', $withundo) or printlog ("Failed to open undo file $withundo: $!\n")
      and $withundo = 0;
    printlog ("Writing undo to $withundo.\n") if $undofile ;
}



my $rewind = 0;
my $from_state = undef;

unless ($reparse) {
  while (<$state>) {
    m/^(\S+)\s+\S+\s+(\d+)/ ;
    $from_state = $1 ;
    $rewind = $2 ;
  }
}

LOG: foreach my $logconf ( keys %{$conf->{confs}} ) {

  # skip all but selected logs with limit
      printlog ("Skipping log $logconf - not selected with limit\n ("
		. join(', ', @limit) . ")\n")
	and next LOG
	  if (scalar @limit and not $limit{$logconf} ) ;

  my $skipped = 0 ;
  my $posted = 0 ;
  my $nocommitmsg = '';


  if ( not $conf->{confs}{$logconf}[0]{logfile} and $conf->{confs}{$logconf}[0]{logcommand}) {
    # we pretend to be generic, but this is really slurm - perhaps we need hooks in logs?
    my $dt = DateTime->today;
    my $yd; # from: this should be the day before last state, or before today
    if ( $from_state
	 and $from_state =~ m/^(\d{4})-(\d{2})-(\d{2})$/
	 and 1971 <= $1 + 0
	 and 1 + 0 <= $dt->year
	 and 1 <= $2 + 0
	 and $2 + 0 <= 12
	 and 1 <= $3 + 0
	 and $3 + 0 <= 31
       ) {
      $yd = DateTime->new( year => $1, month => $2, day => $3 )->subtract(days=>1);
    } else {
      $yd = $dt->clone->subtract(days => 1);
    }
    # use cmdline, state, computed ... in that order
    my ($from, $to) = ($from || $yd->ymd, $to || $dt->ymd);
    my $cmd = "$conf->{confs}{$logconf}[0]{logcommand}";
    my $tmpfile = mktemp('gmjobs_' . $logconf . '_tmp_XXXXX');
    $cmd =~ s/\$from/$from/g;     $cmd =~ s/\$to/$to/g;
    $cmd .= " > $tmpfile";
    printlog("Getting log data for [$logconf]: $cmd ...\n");
    system $cmd or printlog("  $cmd failed:\n $?") ;
    $conf->{confs}{$logconf}[0]{logfile} = $tmpfile;
  }
  if ( open my $log, "<", $conf->{confs}{$logconf}[0]{logfile} )    {

    printlog("Parsing [$logconf]: $conf->{confs}{$logconf}[0]{logfile} ...\n");
    printlog ("Nocommit requested, dryrun (no actual data written to the database or state file).\n") if $nocommit;
    $nocommitmsg = ' (But no commit actually done.)' if $nocommit;

    if ($logconf eq 'slurm.log') {
      printlog("Triggered slurm workaround.\n");

      my $parser = Loggator::Parser->new( $conf->{confs}{$logconf}[0]{patterns} );
      my $matches = 0;
      my %matches = ();
      my $fails = 0;
      my %tags = ();

      my $insEventRecords
	= $dbh->prepare('INSERT IGNORE INTO EventRecords (' .
			'EventID, SiteName, JobName, ' .
			'LocalUserID, LocalUserGroup, ' .
			'WallDuration, CpuDuration, ' . 
			'WallDurationSeconds, CpuDurationSeconds, ' .
			'StartTime, StopTime, StartTimeUTC, StopTimeUTC, ' .
			'StartTimeEpoch, StopTimeEpoch, SubmitHost, ' .
			'MemoryReal, MemoryVirtual, Processed, ' .
			'EventDate, EventTime) ' .
			'VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ? );')
	  or die $dbh->errstr;
      $dbh->{HandleError} = sub { 
	warn "SQL failed with @_\n";
	printlog ("SQL failed with @_\nExiting.\n");
	warn "Exiting.\n";
	die "Exited due to SQL error @_.\n";
      };

      printlog("Slurm processing starts.\n") if $verbose;
      my $prevresult = {}; #for slurm composing

      while (<$log>) {
	my %this_tags = ();
	my ($result, $match, $tags) = $parser->parse($_);
	$matches++ if $match;
	$matches{$match} = defined $matches{$match} ? $matches{$match} + 1 : 1 if $match;
	printlog ("Slurm NOTAGS:\n$_\n" . Dumper($result)) if ($match and not scalar @$tags);
	printlog ("Slurm DAMN: no match!\n$_\n") and $fails++ unless $match;
	foreach (@$tags) {
	  if (exists $tags{$_}) { $tags{$_}++ } else { $tags{$_} = 1} ;
	  $this_tags{$_} = 1;
	}

	# check if this slurm match is suitable
	if ( $match and $match eq 'job'
	     and exists $this_tags{completed} ) {
	  if ( defined $result->{cluster}
	       and defined $result->{cputime}
	       and defined $result->{'end_timestamp'} ) {
	    printlog ("Parsed slurm job: result->{jobid}, $result->{user} || 'NONE', $result->{'start_timestamp'}, $result->{'end_timestamp'}, $nocommitmsg\n") if $verbose;

	  } else {
	    printlog ("SKIPPING STRANGE SLURM OBJECT:\n$_\n" . Dumper(\$result, \%this_tags) . "\n") ;
	    next;
	  }
	  # set up slurm data
	  # we have this in $result->{}:
	  #  jobid submit_timestamp jobname associd account user group cluster cputime
	  #  no_of_cpus no_of_nodes node_list walltime used_cput max_vmem max_rss
	  #  eligible_timestamp start_timestamp end_timestamp status exit_status

          # currently, we compose jobid and jobid.batch until we figure this out
	  if ($result->{jobid} =~ m{^(\d+)[.]batch$}) {
	    printlog("id + id.batch sequence broken in slurm at $1 : $result->{jobid}, skipped.\n") and next
	      unless $prevresult->{jobid} eq $1;
	    foreach my $key (keys %$result) {
	      $result->{$key} = $prevresult->{$key}
		unless defined $result->{$key} and not $result->{$key} eq '';
	    }
	    $result->{jobid} = $prevresult->{jobid};
	  } else {
	    $prevresult = { %$result } and next;
	  }

	  # we don't have a machine id for the eventid
	  my $eventdate = parse_time($result->{'end_timestamp'})->ymd;
	  my $eventtime = parse_time($result->{'end_timestamp'})->hms;
	  $result->{cluster} = 'SiGNET' if $result->{cluster} eq 'signet';
	  my $eventid =  "$eventdate $eventtime $result->{jobid} $batchname $result->{cluster}";
	  $result->{max_rss}  =~ s/K$//;
	  $result->{max_vmem} =~ s/K$//;

	  my @insert = (
			$eventid, $result->{cluster}, $result->{jobid},
			$result->{user}, $result->{group},
			$result->{walltime}, $result->{cputime},
			seconds($result->{walltime}), seconds($result->{cputime}),
			$result->{'start_timestamp'}, $result->{'end_timestamp'},
			time_utc($result->{'start_timestamp'}), time_utc($result->{'end_timestamp'}),
			time_epoch($result->{'start_timestamp'}), time_epoch($result->{'end_timestamp'}),
			$batchname,
			$result->{max_rss}, $result->{max_vmem},
			0, #processed
			$eventdate, $eventtime
		       );

	  # can we at least check
	  if ($hostname and $nocommit) {
	    printlog("Would commit to $hostname: " . join (', ', @insert) . "\n");
	    my $ref = $dbh->selectrow_hashref("SELECT * FROM EventRecords WHERE EventID = '$eventid' ;");
	    if ( $ref ) {
	      printlog ("Skipping existing slurm job $eventid.\n") ;
	      $skipped++ ;
	    } else {
	      print $undofile "DELETE FROM EventRecords WHERE EventID='$eventid';\n";
	      $posted++;
	    }
	  } elsif ($nocommit) {
	    printlog("Would commit: " . join (', ', @insert) . "\n");
	    $posted++;
	    if ($undofile) {
	      print $undofile "DELETE FROM EventRecords WHERE EventID='$eventid';\n";
	    }
	  } else { # commit, really
	    printlog("Commiting: " . join (', ', @insert) . "\n");
	    my $insert =
	      $insEventRecords->execute(@insert);
	    die $dbh->errstr unless $insert;
	    if ($insert == 0) {
	      printlog ("Skipping existing slurm job $eventid.\n") ;
	      $skipped++ ;
	    } else {
	      $posted++
	    }
	    if ($undofile) {
	      print $undofile "DELETE FROM EventRecords WHERE EventID='$eventid';\n";
	    }
	  }
	}
      } # slurm log ended
      printlog ("\t... done. Slurm report:\n");
      $startfrom = DateTime->now() unless defined $startfrom ; #if nothing happened
      $dt = DateTime->now() unless defined $dt ;               #if nothing happened
      printlog ("+ posted $posted, skipped $skipped from " .
		$startfrom->ymd()  . ' ' . $startfrom->hms() . ' to ',
		$dt->ymd()  . ' ' . $dt->hms() . "\n");
    printlog ("+ Parser matches: $matches\t Fails: $fails (" .
      sprintf ("%4.2f", ( $fails + $matches ? ( $fails / ($fails + $matches) * 100) : 0) ). " %)\n") ;
    printlog ("+ Matched patterns:\n");
    foreach (keys %matches) {
      printlog ('+' . sprintf ("%32s:\t%9d (%04.2f %%)\n", $_, $matches{$_}, ($matches{$_} / ($fails + $matches) * 100))) ;
    }
    printlog ("+ Matched tags:\n");
    foreach (keys %tags) {
      printlog ('+' . sprintf ("%32s:\t%9d (%04.2f %%)\n", $_, $tags{$_}, ($tags{$_} / ($fails + $matches) * 100)) ) ;
    }

      next LOG;
    }

    if ($rewind and not $reparse) {
      seek($log, $rewind, 0);
      printlog ("Rewinding $conf->{confs}{$logconf}[0]{logfile} to $rewind.\n");
    } else {
      printlog ("No rewind position to seek to, starting ab initio.\n") unless $rewind;
      printlog ("Reparse requested, starting ab initio.\n") if $reparse;
    }

    if ($jumpto) {
      printlog ("Jumping $conf->{confs}{$logconf}[0]{logfile} to $jumpto by regex scan.\n");
      while (<$log>) {
	next unless m{$jumpto};
	printlog ("\nStarted processing of $conf->{confs}{$logconf}[0]{logfile} at:\n  $_\n");
	last;
      }
    }

    my $parser = Loggator::Parser->new( $conf->{confs}{$logconf}[0]{patterns} );
    my $matches = 0;
    my %matches = ();
    my $fails = 0;
    my %tags = ();
    my $lastpos; my $prepos ; my $preid ;

    my $insBLAHd = $dbh->prepare('INSERT IGNORE INTO BlahdRecords (BlahdID, TimeStamp, GlobalUserName, FullyQualifiedAttributeName, ResourceIdentity, GlobalJobId, LrmsId, JobName, SiteName, ValidFrom, ValidUntil, Processed) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ? );') or die $dbh->errstr;
    $dbh->{HandleError} = sub { 
      warn "SQL failed with @_\n";
      printlog ("SQL failed with @_\nExiting.\n");
      warn "Exiting.\n";
      die "Exited due to SQL error @_.\n";
    };

    printlog("Processing starts.\n") if $verbose;

    while (<$log>) {
      my %this_tags = ();
      my ($result, $match, $tags) = $parser->parse($_);
      $matches++ if $match;
      $matches{$match} = defined $matches{$match} ? $matches{$match} + 1 : 1 if $match;
      printlog ("NOTAGS:\n$_\n" . Dumper($result)) if ($match and not scalar @$tags);
      printlog ("DAMN: no match!\n$_\n") and $fails++ unless $match;
      foreach (@$tags) {
	if (exists $tags{$_}) { $tags{$_}++ } else { $tags{$_} = 1} ;
	$this_tags{$_} = 1;
      }
      #$storage->add($logconf, $match, $result, $tags);
      #and this is a poor man's replacement for jobstat-parser:
      if ( $logconf eq 'gm-jobs.log'
	   and $match eq 'standard'
	   and exists $this_tags{finished}
	 and not exists $this_tags{failed} ) {
 	if ( defined $result->{date}
		 and defined $result->{time}
		 and defined $result->{lrmsid}
		 and defined $result->{ownerDN} ) {
	  my ($day, $month, $year);
	  if ($result->{date} =~ m/^(\d{4})-(\d{2})-(\d{2})$/) {
	    ($day, $month, $year) = ($3, $2, $1);
	  } elsif ($result->{date} =~ m/^(\d{2})-(\d{2})-(\d{4})$/) {
	    ($day, $month, $year) = ($1, $2, $3);
	  } else {
	    printlog ("DAMN: date string not parseable, tried dd-mm-yyyy and yyy-mm-dd!\n$_\n");
	    $fails++;
	  }
	  $dt = new DateTime ( month => $month, day => $day, year => $year );
	  my $dt_before = $dt - new DateTime::Duration( days => 1);
	  my $dt_after  = $dt + new DateTime::Duration( days => 28);
	  my $date   = $dt->ymd() ;
	  my $before = $dt_before->ymd() ;
	  my $after  = $dt_after->ymd()  ;

	  my $jobid = $result->{lrmsid};
	  if ($jobid) {
	    $jobid .= ($dns_suffix ? '.' . $dns_suffix : '') unless $result->{lrmstype} eq 'SLURM';
	  } else {
	    $jobid = 'ERROR';
	  }
	  my $joburi  = $result->{jobid} ? $joburi_prefix . $result->{jobid} : 'ERROR';
	  my $resourceid = $result->{queuename};
	  if ($resourceid) {
	    $resourceid .= ($queuename ? $queuename : '')
	      unless $resourceid eq $queuename;
	  } else {
	    $resourceid = $queuename ? $queuename : 'ERROR';
	  }
	  my $id = "$date $result->{time} $jobid $batchname $sitename";
	  my $blahid = "$date $result->{time} $joburi $resourceid $jobid";
	  my $owner = $result->{ownerDN};

	  #printlog ("Parsed job: $id, $owner, $before, $after. $nocommitmsg\n") if $verbose;
	  $owner = 'NULL' if $hideowner;

	  # FIXTHIS to check if it is in, warn to log (die if several)
	  #         else insertnano
	  if ($hostname and $nocommit) {
	    my $ref = $dbh->selectrow_hashref("SELECT * FROM BlahdRecords WHERE BlahdID = '$blahid' ;");
	    if ( $ref ) {
	      my $vf = $ref->{ValidFrom} ;
	      my $jn = $ref->{LrmsId} ;
	      printlog ("Skipping existing $blahid.\n") ;
	      $skipped++ ;
	    }
	  } else {
	    unless ($nocommit) {
	      # BlahdID, TimeStamp, GlobalUserName, FullyQualifiedAttributeName, ResourceIdentity, GlobalJobId, LrmsId, JobName, SiteName, ValidFrom, ValidUntil, Processed
	      $verbose ?
		printlog ("Inserting new job: $id, $owner, $before, $after. $nocommitmsg\n") :
		  printlog ("Inserted: $id. $nocommitmsg\n") ;
	      my $insert = $insBLAHd->execute($blahid, "$date $result->{time}", $owner, 'NULL', $resourceid, $joburi, $jobid, $jobid, $sitename, $before, $after, 0);
	      die $dbh->errstr unless $insert;
	      if ($insert == 0) {
		printlog ("Skipping existing $blahid.\n") ;
		$skipped++ ;
	      } else {
		$posted++;
		if ($undofile) {
		  print $undofile "DELETE FROM BlahdRecords WHERE BlahdID='$blahid';\n";
		}
	      }
	    } else {
	      $posted++;
	      if ($undofile) {
		print $undofile "DELETE FROM BlahdRecords WHERE BlahdID='$blahid';\n";
	      }
	      $startfrom = $dt if $posted and not defined $startfrom ;
	    }
	  }
	} else {
	printlog ("SKIPPING STRANGE OBJECT:\n$_\n" . Dumper(\$result, \%this_tags) . "\n") ;
      }
      }
      if ($logconf eq 'gm-jobs.log') {
	$prevpos = $lastpos ;
	$lastpos = tell $log;
	$previd = join(' ',
		       $result->{date}, $result->{time},
		       is($result->{lrmsid}), is($result->{ownerDN}) )
      }
    }


    printlog ("\t... done. Report:\n");


    $startfrom = DateTime->now() unless defined $startfrom ; #if nothing happened
    $dt = DateTime->now() unless defined $dt ;               #if nothing happened

    printlog ("+ posted $posted, skipped $skipped from " .
	      $startfrom->ymd()  . ' ' . $startfrom->hms() . ' to ',
	      $dt->ymd()  . ' ' . $dt->hms() . "\n");
    printlog ("+ Parser matches: $matches\t Fails: $fails (" .
      sprintf ("%4.2f", ( $fails + $matches ? ( $fails / ($fails + $matches) * 100) : 0) ). " %)\n") ;
    printlog ("+ Matched patterns:\n");
    foreach (keys %matches) {
      printlog ('+' . sprintf ("%32s:\t%9d (%04.2f %%)\n", $_, $matches{$_}, ($matches{$_} / ($fails + $matches) * 100))) ;
    }
    printlog ("+ Matched tags:\n");
    foreach (keys %tags) {
      printlog ('+' . sprintf ("%32s:\t%9d (%04.2f %%)\n", $_, $tags{$_}, ($tags{$_} / ($fails + $matches) * 100)) ) ;
    }
  }  else  {
    warn "Can't read $conf->{confs}{$logconf}[0]{logfile}, aborting logfile ($!)\n";
    printlog ("Can't read $conf->{confs}{$logconf}[0]{logfile}, aborting logfile ($!)\n");
    next LOG;
  }
}

# commit to db
printlog( "Commiting to the database ...\n");
$dbh->commit() or
  ( printlog( $dbh->errstr() . ", exiting.\n" )
    and die $dbh->errstr() ) ;
$dbh->disconnect() or printlog( $dbh->errstr());
printlog( "Done.\n");
savestate();

sub savestate {
  close $state ;
  if (not defined $prevpos or not defined $rewind or $nocommit) {
    printlog ("No state retained. Exiting.");
    warn "No state retained. Exiting.\n";
    return undef;
  }

  open $state, ">", $statefile
    or die "Can't open state dump file $statefile for writing (no state retainded), @!\n";

  $dt = DateTime->now();
  print $state join(' ',
		   $dt->ymd(), $dt->hms(), defined $prevpos ? $prevpos : $rewind,
		   $previd ? '[' . $previd . ']' : ''), "\n";
  printlog ("Finished at " .
	    (defined $prevpos ? $prevpos : $rewind ) . ' ' .
	    ($previd ? '[' . $previd . ']' : '') . ".\n") ;
}

sub printlog {
  return unless defined $logfile;
  my $dt = DateTime->now();
  print $log $dt->ymd() . ' ' . $dt->hms() . ' ', @_ ;
}

sub is {
  return $_[0] ?  $_[0] : '' ;
}

sub seconds {
  my $t = shift;
  my $days = 0;
  if ($t =~ m{^(\d+)-}) {
    $days = $1 and $t =~ s{^(\d+)-(.*)$}{$2} if $1;
  }
  my @t = reverse split(':', $t);
  return (
	  ($days * 86400) +
	  (($t[2] || 0) * 3600) +
	  (($t[1] || 0) * 60) +
	  ($t[0] || 0)
	 );
}

# handle times for RFC3339-like with no nanoseconds and possibly no timezone 
# 2012-10-01T12:07:47
# 2012-10-01T12:07:47Z+03:00

sub parse_time {
  my $ts = shift;
  $ts =~ m{(\d{4})-(\d{2})-(\d{2})T(\d{2}):(\d{2}):(\d{2})(:?Z(:?([+-]?\d{2}:\d{2}))?)?}x;
  my $tz = 'UTC';
  $tz = "$7" if $7;
  return DateTime->new(
      year       => $1,
      month      => $2,
      day        => $3,
      hour       => $4,
      minute     => $5,
      second     => $6,
      time_zone  => $tz,
   );
}

#function stolen from package DateTime::Format::RFC3339 by Eric Brine
sub format_datetime {
   my $dt = shift;

   my $tz;
   if ($dt->time_zone()->is_utc()) {
      $tz = 'Z';
   } else {
      my $secs  = $dt->offset();
      my $sign = $secs < 0 ? '-' : '+';  $secs = abs($secs);
      my $mins  = int($secs / 60);       $secs %= 60;
      my $hours = int($mins / 60);       $mins %= 60;
      if ($secs) {
         ( $dt = $dt->clone() )
            ->set_time_zone('UTC');
         $tz = 'Z';
      } else {
         $tz = sprintf('%s%02d:%02d', $sign, $hours, $mins);
      }
   }

   return
      $dt->strftime(
         ($dt->nanosecond()
            ? '%Y-%m-%dT%H:%M:%S.%9N'
            : '%Y-%m-%dT%H:%M:%S'
         )
      ).$tz;
}

sub time_utc {
  return format_datetime(parse_time(shift)->set_time_zone('UTC'));
}
sub time_epoch {
  return parse_time(shift)->epoch;
}

sub usage {
  print <<"FNORD" ;
\u$progname (version $version): Incrementaly read new items in NG
ARC's gm-jobs log and post them to the gLite's MySQL data base to be
handled by apel publisher.

\u$progname tries not to repost anything.


The last position read in the log is kept in a statefile between runs.
Grid job entries are fist looked up in the database and only posted if
not yet present.

Warning: \u$progname sets any log member variable to ERROR if it is
missing when it expects it to be set, but currently does not report
this in any way.

Warning: \u$progname currently parses loggator backend config files,
but does not use them. This should be added soon.

Options:

 --config    -c  Set Loggator configuration directory.
                 See Loggator::Config. Default value: '$config'.
 --logfile   -l  Logfile location for gm-jobs-postmetrics.
                 If empty, no log is kept. Default value: '$logfile'.
 --statefile -s  Statefile where last byte position read and last line
                 is kept between runs. Default: '$statefile'.
                 If empty, no state is preserved and the log is always
                 parsed from the start, which is slow.
 --jumpto -j     The argument should be a fixed string or a regular
                 expression.  The parser will jump to the location
                 specified by the state file if any, then skip forward and
                 only start parsing when a match for the string or
                 expression is found. This can be used when a
                 statefile is not generated after a failure to speed
                 up the system.
 --blahmode  -B  Obsoleted compatibiliy switch for newer database format.
                 This is now the default, and the old mode is disabled.
 --limit  -L     Limit the processing only to the stated log file configuration.
                 Can be used multiple times to process only selected log files.
                 Alternatively, a comma-separated list can be used.
 --nocommit  -n  Do nothing, just parse the log file and check
                 for duplicates in the database.
 --reparse   -r  Ignore the state file, start from the beginning
                 of the log.
 --from --to     For those sources that support it (SLURM), use
                 the given from and to time parameters to capture logged events
                 instead of state or computed values (one or both can be specified)
 --verbose   -v  Print info on STDERR while running.
 --debug     -d  Print more info.
 --withundo  -u  Write undo files to specified file.
                 This will append to the file if it exists.
                 A %d and a %t in the filename will be replaced by
                 current date and time in the format of yyyy-mm-dd hh-mm-ss.
 --help      -h  Prints this help.

Following are the backend database settings, settable also in the backends
config file. Options take precedence.

 --hostname  -H  Hostname of MySQL server for gLite/apel.
                 If empty, no checking and no posting is performed.
 --port      -p  Port for MySQL server
 --database  -b  MySQL server database.
                 Default: '$database'.
 --username  -U  MySQL remote user.
                 Default: '$user'.
 --password  -P  MySQL password. (Passwords on command line are insecure!)
 --transaction -t  The number of insertions to put in a single transaction.
                 If not set, everything is commited at the end.
                 Default: '$transaction'.

Following are the site settings, settable aslo in the site config file.
Options take precedence.

 --sitename      Descriptive name of the site.
 --dns_suffix    The DNS suffix of the site - used in some kinds of LRMS logs
                 when ommited in gmjobs.log. This must correspond to the
                 LRMS logs for the LRMS job ids to match.
                 Needed for PBS-like LRMS, for example.
 --joburi_prefix Used to construct the job uri data; for ARC jobs, a gridftp
                 uri of the job on the ARC frontend is customary, with the
                 prefix in this format:
                   gsiftp://arcfrontend.example.com:2811/jobs/
 --queuename     Frontend name with the \@ prefix, to construct the queuename
                 in the format queueanem\@arcfrontend.example.com.
                 Should start with '\@'. Ignored if not set, empty or equal
                 to an actua queue.
 --batchname     DNS name for the batch manager. Needed to match some LRMS
                 records, such as used by PBS and PBS-like LRMS
 --hideowner     If set, NULL is reported instead of the Distinguished Name
                 of the job owner


Example:

  $progname -c /etc/$progname/conf.d -H my.host.org -p 3306 \
     -b accounting -U accounting -P password123 -t 200\
     --sitename SiGNET --dns_suffix .ijs.si \
     --joburi_prefix gsiftp://pikolit.ijs.si:2811/jobs/ \
     --queuename \@pikolit.ijs.si --batchname brenta.ijs.si \
     -l /var/log/$progname -s /var/state/$progname \
     -u /var/state/$progname-undo-%d-%t 
FNORD
}
